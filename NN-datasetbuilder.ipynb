{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8700e36-fb5e-4267-bbaf-2d5813742569",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "import secretflow as sf\n",
    "from secretflow.security.aggregation import SPUAggregator, SecureAggregator\n",
    "from secretflow.ml.nn import FLModel\n",
    "from secretflow.utils.simulation.datasets import dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from secretflow.utils.simulation.data.ndarray import create_ndarray\n",
    "\n",
    "\n",
    "# Check the version of your SecretFlow\n",
    "print(\"The version of SecretFlow: {}\".format(sf.__version__))\n",
    "\n",
    "# In case you have a running secretflow runtime already.\n",
    "sf.shutdown()\n",
    "\n",
    "sf.init([\"alice\", \"bob\", \"charlie\"], address=\"local\")\n",
    "alice, bob, charlie = sf.PYU(\"alice\"), sf.PYU(\"bob\"), sf.PYU(\"charlie\")\n",
    "spu = sf.SPU(sf.utils.testing.cluster_def([\"alice\", \"bob\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b84680-6616-4d54-8582-bf753b71c01d",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset(\"creditcard\"), sep=\",\")\n",
    "sf_df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cf60f-7db2-463a-98f7-08c0ff9111ec",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "half = len(sf_df) // 2\n",
    "h_alice = sf_df.iloc[:half]\n",
    "h_bob = sf_df.iloc[half:]\n",
    "# Save to temorary files.\n",
    "_, h_alice_path = tempfile.mkstemp()\n",
    "_, h_bob_path = tempfile.mkstemp()\n",
    "h_alice.to_csv(h_alice_path, index=False)\n",
    "h_bob.to_csv(h_bob_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e725618-98c1-477f-9ba0-39eacfd4b1c1",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "def create_dataset_builder(\n",
    "    batch_size = 256,\n",
    "):\n",
    "    def dataset_builder(folder_path, stage=\"train\"):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import tensorflow as tf\n",
    "        import math\n",
    "        pd_data = pd.read_csv(folder_path)\n",
    "        data = np.array(pd_data, dtype=np.float32)  \n",
    "\n",
    "        # 预处理数据\n",
    "        X = data[:, :-1]\n",
    "        y = data[:, -1]  \n",
    "        # 对特征进行Sigmoid操作\n",
    "        X[:, 0] = 1 / (1 + np.exp(-X[:, 0]))\n",
    "        X[:, -1] = 1 / (1 + np.exp(-X[:, -1]))\n",
    "\n",
    "        # 使用SMOTE进行过采样\n",
    "        smote = SMOTE()\n",
    "        X, y = smote.fit_resample(X, y)\n",
    "\n",
    "        # 拆分训练集和测试集\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1234)\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "        if stage == \"train\":\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "            train_dataset_size = len(X_train[0])\n",
    "            train_step_per_epoch = math.ceil(train_dataset_size / batch_size)\n",
    "            return train_dataset, train_step_per_epoch\n",
    "        elif stage == \"eval\":\n",
    "            eval_dataset =  tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "            eval_dataset_size = len(X_test[0])\n",
    "            eval_step_per_epoch = math.ceil((eval_dataset_size) / batch_size)\n",
    "            return eval_dataset, eval_step_per_epoch\n",
    "\n",
    "    return dataset_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e8c142-cd4c-4861-a8bf-a0895498ac03",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "data_builder_dict = {\n",
    "    alice: create_dataset_builder(\n",
    "        batch_size=256,\n",
    "    ),\n",
    "    bob: create_dataset_builder(        \n",
    "        batch_size=256,\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c078b4a-92ee-4b80-8d17-2a16c6626ecf",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    alice: h_alice_path,\n",
    "    bob: h_bob_path,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0033e-c051-47f9-8993-87ca82609da3",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "\n",
    "device_list = [alice, bob]\n",
    "secure_aggregator = SecureAggregator(charlie, [alice, bob])\n",
    "spu_aggregator = SPUAggregator(spu)\n",
    "num_classes = 10\n",
    "input_shape = h_alice[0].shape\n",
    "model = create_model(input_shape, num_classes)\n",
    "fed_model = FLModel(\n",
    "    server=charlie,\n",
    "    device_list=device_list,\n",
    "    model=model,\n",
    "    aggregator=secure_aggregator,\n",
    "    strategy=\"fed_avg_w\",\n",
    "    backend=\"tensorflow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7ab61-44f5-45bb-8061-6b426a41c376",
   "metadata": {
    "execution": {},
    "libroFormatter": "formatter-string"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "history = fed_model.fit(\n",
    "    data,\n",
    "    None,\n",
    "    validation_data=data,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    aggregate_freq=1,\n",
    "    verbose=1,\n",
    "    dataset_builder=data_builder_dict,\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
